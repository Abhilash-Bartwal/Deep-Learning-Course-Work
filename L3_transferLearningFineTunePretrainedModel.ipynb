{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHbVPl4yUnqI",
        "outputId": "163fcbcc-221e-4b52-c84c-ad32b5c56bea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 165ms/step - accuracy: 0.1424 - loss: 2.2368 - val_accuracy: 0.2217 - val_loss: 1.9695\n",
            "Epoch 2/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 158ms/step - accuracy: 0.1462 - loss: 2.1420 - val_accuracy: 0.2149 - val_loss: 1.9981\n",
            "Epoch 3/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 158ms/step - accuracy: 0.1537 - loss: 2.1290 - val_accuracy: 0.2328 - val_loss: 1.9320\n",
            "Epoch 4/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 158ms/step - accuracy: 0.1559 - loss: 2.1194 - val_accuracy: 0.2118 - val_loss: 1.9999\n",
            "Epoch 5/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 158ms/step - accuracy: 0.1566 - loss: 2.1232 - val_accuracy: 0.2294 - val_loss: 1.9782\n",
            "Epoch 6/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 158ms/step - accuracy: 0.1599 - loss: 2.1061 - val_accuracy: 0.2331 - val_loss: 1.9567\n",
            "Epoch 7/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 158ms/step - accuracy: 0.1569 - loss: 2.1169 - val_accuracy: 0.2226 - val_loss: 1.9759\n",
            "Epoch 8/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 158ms/step - accuracy: 0.1577 - loss: 2.1143 - val_accuracy: 0.2376 - val_loss: 1.9402\n",
            "Epoch 9/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 158ms/step - accuracy: 0.1593 - loss: 2.1084 - val_accuracy: 0.2381 - val_loss: 1.9341\n",
            "Epoch 10/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 158ms/step - accuracy: 0.1616 - loss: 2.1012 - val_accuracy: 0.2200 - val_loss: 1.9730\n",
            "Epoch 1/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 227ms/step - accuracy: 0.0970 - loss: 2.3477 - val_accuracy: 0.1000 - val_loss: 2.3369\n",
            "Epoch 2/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 199ms/step - accuracy: 0.0968 - loss: 2.3333 - val_accuracy: 0.1000 - val_loss: 2.3251\n",
            "Epoch 3/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 199ms/step - accuracy: 0.0968 - loss: 2.3225 - val_accuracy: 0.1000 - val_loss: 2.3171\n",
            "Epoch 4/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 199ms/step - accuracy: 0.0968 - loss: 2.3153 - val_accuracy: 0.1000 - val_loss: 2.3117\n",
            "Epoch 5/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 199ms/step - accuracy: 0.0968 - loss: 2.3105 - val_accuracy: 0.1000 - val_loss: 2.3081\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model training and fine-tuning complete.\n"
          ]
        }
      ],
      "source": [
        " # Import necessary libraries\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_val, y_val) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to the range [0, 1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_val = x_val.astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "num_classes = 10\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_val = to_categorical(y_val, num_classes)\n",
        "\n",
        "# Define a data pipeline with dynamic resizing and batching\n",
        "def preprocess(image, label):\n",
        "    image = tf.image.resize(image, [224, 224])  # Resize to VGG16 input size\n",
        "    return image, label\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_ds = train_ds.map(preprocess).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_ds = val_ds.map(preprocess).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Load the pre-trained VGG16 model without the top layer\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze all the layers in the base model to retain pre-trained weights\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom layers for our specific classification task\n",
        "x = Flatten()(base_model.output)  # Flatten the output of the base model\n",
        "x = Dense(128, activation='relu')(x)  # Add a dense layer with 128 neurons\n",
        "x = Dropout(0.5)(x)  # Add dropout for regularization\n",
        "output = Dense(num_classes, activation='softmax')(x)  # Final output layer\n",
        "\n",
        "# Create the complete model\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with frozen base layers\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=10)\n",
        "\n",
        "# Unfreeze some layers of the base model for fine-tuning\n",
        "for layer in base_model.layers[-4:]:  # Unfreeze the last 4 layers\n",
        "    layer.trainable = True\n",
        "\n",
        "# Recompile the model after unfreezing\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fine-tune the model\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=5)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save('fine_tuned_vgg16_cifar10.h5')\n",
        "\n",
        "print(\"Model training and fine-tuning complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lecZWFMUUyN_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}